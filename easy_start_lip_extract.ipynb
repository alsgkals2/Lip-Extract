{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ca76ec3-6ff1-410d-9b3d-51fead5bca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-process dataset by extracting aligned cropped mouths. Adapted from https://github.com/mpc001/\n",
    "Lipreading_using_Temporal_Convolutional_Networks/blob/master/preprocessing/crop_mouth_from_video.py\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import warp_img, apply_transform, cut_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e033fd42-e23a-43ed-ae17-a96cdc4d23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"FaceForensics++\": [\n",
    "        \"Forensics/RealFF\",\n",
    "        \"Forensics/Deepfakes\",\n",
    "        \"Forensics/FaceSwap\",\n",
    "        \"Forensics/Face2Face\",\n",
    "        \"Forensics/NeuralTextures\",\n",
    "    ],\n",
    "    \"RealFF\": [\"Forensics/RealFF\"],\n",
    "    \"Deepfakes\": [\"Forensics/Deepfakes\"],\n",
    "    \"FaceSwap\": [\"Forensics/FaceSwap\"],\n",
    "    \"Face2Face\": [\"Forensics/Face2Face\"],\n",
    "    \"NeuralTextures\": [\"Forensics/NeuralTextures\"],\n",
    "    \"FaceShifter\": [\"Forensics/FaceShifter\"],\n",
    "    \"DeeperForensics\": [\"Forensics/DeeperForensics\"],\n",
    "    \"CelebDF\": [\"CelebDF/RealCelebDF\", \"CelebDF/FakeCelebDF\"],\n",
    "    \"DFDC\": [\"DFDC\"],\n",
    "}\n",
    "STD_SIZE = (256, 256)\n",
    "STABLE_POINTS = [33, 36, 39, 42, 45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d429b-d6cf-4b92-9d79-9d013e0a87d5",
   "metadata": {},
   "source": [
    "### PREPROCESSING (LOAD THE LANDMARK WHICH IS EXTRACTED PREIVIOUSLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacc6fc1-a9d0-4fcd-a01e-08e29865f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('land_fake_train.json', 'r') as f:\n",
    "    land_fake =  json.load(f)\n",
    "print(len(land_fake))\n",
    "\n",
    "import json\n",
    "with open('land_real_train.json', 'r') as f:\n",
    "    land_real =  json.load(f)\n",
    "print(len(land_real))\n",
    "landmarks=dict()\n",
    "\n",
    "for img, target in tqdm(zip(list_files, list_files_target)):\n",
    "    IS_RUN = False\n",
    "    if 'FAKE/' in img :\n",
    "        if img in land_fake:\n",
    "            landmarks[img]=land_fake[img]\n",
    "    else :\n",
    "        if img in land_real:\n",
    "            landmarks[img]=land_real[img]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "06f5ab5d-9757-460b-af1b-9030f53fbee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120281/120281 [18:27<00:00, 108.62it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "from utils import warp_img, apply_transform, cut_patch\n",
    "\n",
    "q_landmarks = deque()\n",
    "q_frames = deque()\n",
    "q_name = deque()\n",
    "for land in tqdm.tqdm(landmarks):\n",
    "    with Image.open(land) as pil_img:\n",
    "        img = np.array(pil_img)\n",
    "\n",
    "    # Add elements to the queues\n",
    "    q_frames.append(img)\n",
    "    q_landmarks.append(landmarks[land])\n",
    "    q_name.append(os.path.basename(land))\n",
    "\n",
    "    if len(q_frames) == 12:  # Wait until queues are large enough\n",
    "        smoothed_landmarks = np.mean(q_landmarks, axis=0)\n",
    "\n",
    "        cur_landmarks = q_landmarks.popleft()\n",
    "        cur_landmarks = np.array(cur_landmarks)\n",
    "        cur_frame = q_frames.popleft()\n",
    "        cur_name = q_name.popleft()\n",
    "\n",
    "        # Get aligned frame as well as affine transformation that produced it\n",
    "        trans_frame, trans = warp_img(\n",
    "            smoothed_landmarks[STABLE_POINTS, :], cur_landmarks[STABLE_POINTS, :], cur_frame, STD_SIZE\n",
    "        )\n",
    "\n",
    "        # Apply that affine transform to the landmarks\n",
    "        trans_landmarks = trans(cur_landmarks)\n",
    "\n",
    "        # Crop mouth region\n",
    "        cropped_frame = cut_patch(\n",
    "            trans_frame,\n",
    "            trans_landmarks[48 : 68],\n",
    "            96 // 2,\n",
    "            96 // 2,\n",
    "        )\n",
    "\n",
    "        # Save image\n",
    "        target_path = land.replace('FRAMES_PNG','LIPS_PNG')\n",
    "        os.makedirs(os.path.dirname(target_path),exist_ok=True)\n",
    "#             target_path = os.path.join(target_dir, cur_name)\n",
    "        Image.fromarray(cropped_frame.astype(np.uint8)).save(target_path)\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
